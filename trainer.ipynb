{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import DEBUG, INFO, FileHandler, Formatter, Logger, StreamHandler, getLogger\n",
    "from pathlib import Path\n",
    "\n",
    "_LOGGIN_DIR = Path().parent / \"logs\"\n",
    "\n",
    "if not _LOGGIN_DIR.exists():\n",
    "    _LOGGIN_DIR.mkdir()\n",
    "\n",
    "\n",
    "def create_logger(name: str) -> Logger:\n",
    "    logger = getLogger(name)\n",
    "    logger.setLevel(DEBUG)\n",
    "    stream_handler = StreamHandler()\n",
    "    file_handler = FileHandler(_LOGGIN_DIR / f\"{name}.log\")\n",
    "    formatter = Formatter(fmt=\"%(asctime)s.%(msecs)03d %(levelname)s: %(message)s\", datefmt=\"%Y-%m-%d,%H:%M:%S\")\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    stream_handler.setLevel(INFO)\n",
    "    file_handler.setLevel(DEBUG)\n",
    "    logger.addHandler(stream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha256\n",
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "from typing import Optional\n",
    "from urllib.request import urlretrieve\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "_DATASETS_DIR = Path().parent / \"datasets\"\n",
    "\n",
    "if not _DATASETS_DIR.exists():\n",
    "    _DATASETS_DIR.mkdir()\n",
    "\n",
    "logger = create_logger(__name__)\n",
    "\n",
    "\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b: int = 1, bsize: int = 1, tsize: int = None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "\n",
    "def load_dataset(name: str, url: str, hash: Optional[str] = None) -> None:\n",
    "    directory_name = _DATASETS_DIR / name\n",
    "    archive_name = directory_name.with_suffix(\".zip\")\n",
    "    if not archive_name.exists():\n",
    "        logger.info(f\"Downloading dataset '{url}' into {archive_name}...\")\n",
    "        with DownloadProgressBar(unit=\"B\", unit_scale=True, miniters=1, desc=name) as t:\n",
    "            urlretrieve(url, archive_name, t.update_to)\n",
    "    else:\n",
    "        logger.debug(f\"Dataset '{url}' is found in {archive_name}!\")\n",
    "    if hash is not None:\n",
    "        logger.debug(f\"Verifying dataset {name} archive {archive_name} SHA256 checksum...\")\n",
    "        if sha256(archive_name.read_bytes()).hexdigest() == hash:\n",
    "            logger.info(f\"Dataset {name} SHA256 verification successful!\")\n",
    "        else:\n",
    "            raise ValueError(f\"Error verifying dataset {name} archive {archive_name} SHA256 checksum!\")\n",
    "    if directory_name.is_dir():\n",
    "        logger.debug(f\"Removing previous dataset {name} directory...\")\n",
    "        rmtree(directory_name)\n",
    "    with ZipFile(archive_name, \"r\") as zipfile:\n",
    "        logger.debug(f\"Unpacking dataset {name} into {_DATASETS_DIR}...\")\n",
    "        zipfile.extractall(directory_name)\n",
    "    logger.info(f\"Dataset {name} available in {directory_name}!\")\n",
    "\n",
    "\n",
    "def verify_dataset(name: str) -> None:\n",
    "    directory_name = _DATASETS_DIR / name\n",
    "    if directory_name.is_dir():\n",
    "        logger.info(f\"Dataset {name} found in {directory_name}!\")\n",
    "    else:\n",
    "        raise RuntimeError(f\"Dataset {name} does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "raXZgluvhdUQ",
    "outputId": "0ab6fb00-2dd7-448c-959f-6c5420af6031"
   },
   "outputs": [],
   "source": [
    "load_dataset(\n",
    "    \"allergen30\",\n",
    "    \"https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/9ygs9vhnpw-1.zip\",\n",
    "    \"ab6e19d32f7490988ca77d600fc6f3df2e8648365c4c92ced8c1b462c01d9d9f\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset(\n",
    "    \"kaggle_calories\",\n",
    "    \"https://www.kaggle.com/api/v1/datasets/download/kkhandekar/calories-in-food-items-per-100-grams\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "beqgkS8ZhdUW",
    "outputId": "c3f497eb-df58-4c4a-b949-3928ecb0a8f3"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "YOLO_VERSION = \"yolo11n.pt\"\n",
    "\n",
    "\n",
    "model = YOLO(YOLO_VERSION)\n",
    "model.model_name = \"find_allergens\"\n",
    "\n",
    "print(\"YOLO network parameters:\")\n",
    "for k, v in model.named_parameters():\n",
    "  print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "IMAGE_SIZE = 416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GeS8GRuIhdUX",
    "outputId": "667a1dca-6b75-4ab0-fcde-8941b0302312"
   },
   "outputs": [],
   "source": [
    "FREEZE_LAYERS = 10\n",
    "EPOCHS_NUMBER = 15\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "\n",
    "model.train(\n",
    "    data=\"datasets/allergen30.yaml\",\n",
    "    epochs=EPOCHS_NUMBER,\n",
    "    batch=BATCH_SIZE,\n",
    "    freeze=FREEZE_LAYERS,\n",
    "    imgsz=IMAGE_SIZE,\n",
    "    lr0=LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dpj7GF7MhdUY",
    "outputId": "b756fdaa-16cc-45bc-e3d7-c967f7584cb3"
   },
   "outputs": [],
   "source": [
    "EPOCHS_NUMBER = 7\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "\n",
    "model.train(\n",
    "    data=\"datasets/allergen30.yaml\",\n",
    "    epochs=EPOCHS_NUMBER,\n",
    "    batch=BATCH_SIZE,\n",
    "    imgsz=IMAGE_SIZE,\n",
    "    lr0=LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FfbaW6gbhdUZ",
    "outputId": "36e0a24f-93dc-4de3-9f2b-361fbdaee896"
   },
   "outputs": [],
   "source": [
    "results = model.predict(source=\"datasets/allergen30/Allergen30/test/images\", show_labels=True, conf=0.25)\n",
    "print(\"Test set image predictions:\")\n",
    "for r in results:\n",
    "    print(r.boxes.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-sWZcVehdUa"
   },
   "outputs": [],
   "source": [
    "model.save(\"find_allergens.pt\")\n",
    "model.export(format=\"onnx\", imgsz=IMAGE_SIZE, optimize=True, simplify=True, int8=True, data=\"datasets/allergen30.yaml\", nms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import PathLike\n",
    "from typing import List, Tuple, Union, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "from cv2 import imread, cvtColor, resize, COLOR_BGR2RGB\n",
    "from pandas import read_csv\n",
    "from torch import Tensor, load, device\n",
    "from torch import max, argmax\n",
    "from torch.cuda import is_available\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "class AllergenCaloriesPredictor:\n",
    "    def __init__(self, model_file: Union[str, bytes, PathLike], calories_dataset: Union[str, bytes, PathLike]) -> None:\n",
    "        model_file = Path(model_file)\n",
    "        if not model_file.exists():\n",
    "            raise RuntimeError(f\"Model file '{model_file}' not found!\")\n",
    "        calories_dataset = Path(calories_dataset)\n",
    "        if not calories_dataset.exists():\n",
    "            raise RuntimeError(f\"Calories dataset '{calories_dataset}' not found!\")\n",
    "\n",
    "        dev = device(\"cuda\") if is_available() else device(\"cpu\")\n",
    "        self._model = load(model_file.absolute(), dev, weights_only=False)[\"model\"]\n",
    "        self._model.eval()\n",
    "        self._calories = read_csv(calories_dataset)\n",
    "        labels = [label.capitalize() for label in self._model.names.values()]\n",
    "        self._calories = self._calories[self._calories[\"FoodItem\"].isin(labels)]\n",
    "\n",
    "    def _prepare_image(self, image_path: Union[str, bytes, PathLike]) -> Tensor:\n",
    "        image = imread(image_path)\n",
    "        image = cvtColor(image, COLOR_BGR2RGB)\n",
    "        image = resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        image_tensor = ToTensor()(image)\n",
    "        return image_tensor.unsqueeze(0).half()\n",
    "\n",
    "    def _parse_yolo_output(self, outputs, conf_threshold: float = 0.5) -> Tuple[Tensor, Tensor]:\n",
    "        detection = outputs[0].view(1, 34, -1)\n",
    "        class_confidences = detection[:, 5:, :]\n",
    "        filter_mask = max(class_confidences, dim=1)[0] > conf_threshold\n",
    "        filtered_boxes = detection[:, :4, :].permute(0, 2, 1)[filter_mask].view(-1, 4)\n",
    "        filtered_classes = class_confidences.permute(0, 2, 1)[filter_mask].view(-1, class_confidences.size(1))\n",
    "        return filtered_boxes, filtered_classes\n",
    "\n",
    "    def predict(self, image_path: Union[str, bytes, PathLike], confidence: float = 0.5, top: int = 3) -> List[Tuple[str, Optional[str], float, Tuple[float, float, float, float]]]:\n",
    "        prediction = self._model(self._prepare_image(image_path))\n",
    "        fboxes, fclasses = self._parse_yolo_output(prediction[0], confidence)\n",
    "        fargmaxes = [argmax(fc).item() for fc in fclasses]\n",
    "        result = list()\n",
    "        for fa, fc, fb in zip(fargmaxes, fclasses, fboxes):\n",
    "            label = self._model.names[fa + 1]\n",
    "            calories = self._calories[self._calories[\"FoodItem\"] == label.capitalize()]\n",
    "            calories = None if len(calories) == 0 else calories.iloc[0][\"Cals_per100grams\"]\n",
    "            confidence = fc[fa].item()\n",
    "            location = tuple(fb.tolist())\n",
    "            result += [(label, calories, confidence, location)]\n",
    "        return sorted(result, key=lambda r: r[2], reverse=True)[:top]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avopic = \"datasets/allergen30/Allergen30/test/images/00000855_jpg.rf.aae89d3987f6e278cb471a0ad94aeb70.jpg\"\n",
    "acp = AllergenCaloriesPredictor(\"find_allergens.pt\", \"datasets/kaggle_calories/calories.csv\")\n",
    "for pclass, cals, pconf, pbox in acp.predict(avopic, 0.75):\n",
    "    print(f\"Object found: {pclass}\\n\\tconfidence: {pconf}\\n\\tcalories: {cals if cals is not None else 'unknown :('}\\n\\tat: {pbox}\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "imagine-food-lMcR2JK_-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
